{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import json\n",
    "import pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the annotations file\n",
    "annotations_file = 'annotation/ADT_1_0.txt'\n",
    "\n",
    "# Define the path to the image directory\n",
    "image_dir = 'annotation/ADT_1_0.png'\n",
    "\n",
    "# Define the categories\n",
    "categories = [\n",
    "    {'id': 0, 'name': 'nucleus'},\n",
    "]\n",
    "\n",
    "# Define the output file path\n",
    "output_file = 'annotation/ADT_1_0.json'\n",
    "\n",
    "# Initialize the annotations dictionary\n",
    "annotations = {\n",
    "    'images': [],\n",
    "    'annotations': [],\n",
    "    'categories': categories\n",
    "}\n",
    "\n",
    "# Read the annotations file\n",
    "with open(annotations_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Process each line of the annotations file\n",
    "for line in lines:\n",
    "    # Parse the line to extract the image file name, bounding box coordinates, and category label\n",
    "    category, x1, y1, x2, y2, = line.strip().split()\n",
    "    \n",
    "    # Add the image information to the annotations dictionary\n",
    "    image_id = len(annotations['images']) + 1\n",
    "    image_info = {\n",
    "        'id': image_id,\n",
    "        #'file_name': image_file,\n",
    "        'width': 0,  # Set to the actual width of the image\n",
    "        'height': 0  # Set to the actual height of the image\n",
    "    }\n",
    "    annotations['images'].append(image_info)\n",
    "    \n",
    "    # Add the annotation information to the annotations dictionary\n",
    "    annotation_id = len(annotations['annotations']) + 1\n",
    "    annotation_info = {\n",
    "        'id': annotation_id,\n",
    "        'image_id': image_id,\n",
    "        'category_id': int(category),\n",
    "        'bbox': [float(x1), float(y1), float(x2) - float(x1), float(y2) - float(y1)],\n",
    "        'area': (float(x2) - float(x1)) * (float(y2) - float(y1)),\n",
    "        'iscrowd': 0\n",
    "    }\n",
    "    annotations['annotations'].append(annotation_info)\n",
    "\n",
    "# Write the annotations to the output file\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(annotations, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "WARNING:tensorflow:From /Users/cusniwtt/opt/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-19 16:46:34.555342: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-19 16:46:34.555365: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the annotations file\n",
    "annotations_file = 'annotation/ADT_1_0.json'\n",
    "\n",
    "# Define the path to the image directory\n",
    "image_dir = '/path/to/images/'\n",
    "image_dir = 'annotation/ADT_1_0.png'\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 8\n",
    "\n",
    "# Define the image size\n",
    "image_size = (256, 256)\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess(image, labels):\n",
    "    # Resize the image\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    \n",
    "    # Normalize the image\n",
    "    image = tf.keras.applications.resnet50.preprocess_input(image)\n",
    "    \n",
    "    # Return the preprocessed image and labels\n",
    "    return image, labels\n",
    "\n",
    "# Define the dataset\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: pycocotools.coco.COCO(annotations_file).imgs.values(),\n",
    "    output_types=(tf.string, tf.float32, tf.float32),\n",
    "    output_shapes=(tf.TensorShape([]), tf.TensorShape([None, 4]), tf.TensorShape([None, None])),\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda image_file, bbox, masks: (tf.io.read_file(image_dir + image_file), bbox, masks),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda image, bbox, masks: (tf.image.decode_jpeg(image), bbox, masks),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda image, bbox, masks: preprocess(image, (bbox, masks)),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "dataset = dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoILayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size, **kwargs):\n",
    "        self.pool_size = pool_size\n",
    "        super(RoILayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        feature_map, rois = inputs\n",
    "\n",
    "        # Get the shape of the feature map\n",
    "        feature_map_shape = tf.shape(feature_map)\n",
    "        feature_map_height = feature_map_shape[1]\n",
    "        feature_map_width = feature_map_shape[2]\n",
    "\n",
    "        # Normalize the RoIs to be in the range [0, 1]\n",
    "        normalized_rois = tf.cast(rois, tf.float32) / tf.constant([feature_map_height, feature_map_width, feature_map_height, feature_map_width])\n",
    "\n",
    "        # Crop and resize the feature map for each RoI\n",
    "        cropped_rois = tf.image.crop_and_resize(feature_map, normalized_rois, tf.zeros([tf.shape(rois)[0]], dtype=tf.int32), [self.pool_size, self.pool_size])\n",
    "\n",
    "        return cropped_rois\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(RoILayer, self).get_config()\n",
    "        config.update({'pool_size': self.pool_size})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256, 256, 3)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb#W2sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39minput_shape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb#W2sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mprint\u001b[39m(inputs\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb#W2sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m proposals, rpn_class_logits, rpn_bbox \u001b[39m=\u001b[39m rpn(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb#W2sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m classes, boxes \u001b[39m=\u001b[39m backbone(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb#W2sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m mask_logits \u001b[39m=\u001b[39m mask_net(inputs, proposals)\n",
      "\u001b[1;32m/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb Cell 5\u001b[0m in \u001b[0;36mrpn\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Generate proposals\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m proposals \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(images\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Convert class probabilities to objectness scores\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     objectness \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msqueeze(class_logits[i, :, :, \u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cusniwtt/Documents/GitHub/deep-learning-cell-counting/maskRCNN_chatGPT.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# Generate bounding box proposals using non-maximum suppression\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# Define the backbone network\n",
    "backbone = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')\n",
    "\n",
    "# Define the region proposal network\n",
    "rpn_conv = tf.keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='rpn_conv')\n",
    "rpn_class = tf.keras.layers.Conv2D(2, (1, 1), activation='sigmoid', name='rpn_class')\n",
    "rpn_bbox = tf.keras.layers.Conv2D(4, (1, 1), activation='linear', name='rpn_bbox')\n",
    "\n",
    "def rpn(images):\n",
    "    x = rpn_conv(images)\n",
    "    class_logits = rpn_class(x)\n",
    "    bbox_pred = rpn_bbox(x)\n",
    "    # Generate proposals\n",
    "    proposals = []\n",
    "    for i in range(images.shape[0]):\n",
    "        # Convert class probabilities to objectness scores\n",
    "        objectness = tf.squeeze(class_logits[i, :, :, 1])\n",
    "        \n",
    "        # Generate bounding box proposals using non-maximum suppression\n",
    "        proposals_i = tf.image.combined_non_max_suppression(\n",
    "            tf.expand_dims(bbox_pred[i], axis=0), \n",
    "            tf.expand_dims(objectness, axis=0), \n",
    "            max_output_size_per_class=100, \n",
    "            max_total_size=100, \n",
    "            iou_threshold=0.5, \n",
    "            score_threshold=0.5\n",
    "        )\n",
    "        \n",
    "        # Append proposals to the list\n",
    "        proposals.append(proposals_i)\n",
    "        \n",
    "    return proposals, class_logits, bbox_pred\n",
    "\n",
    "# Define the mask prediction network\n",
    "mask_conv1 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='mask_conv1')\n",
    "mask_conv2 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='mask_conv2')\n",
    "mask_conv3 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='mask_conv3')\n",
    "mask_conv4 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='mask_conv4')\n",
    "mask_deconv = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), activation='relu', name='mask_deconv')\n",
    "mask_logits = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid', name='mask_logits')\n",
    "\n",
    "def mask_net(images, proposals):\n",
    "    x = tf.image.crop_and_resize(images, proposals, box_indices=tf.range(tf.shape(images)[0]), crop_size=(14, 14))\n",
    "    x = mask_conv1(x)\n",
    "    x = mask_conv2(x)\n",
    "    x = mask_conv3(x)\n",
    "    x = mask_conv4(x)\n",
    "    x = mask_deconv(x)\n",
    "    mask_logits = mask_logits(x)\n",
    "    return mask_logits\n",
    "\n",
    "# Define the loss function\n",
    "def mask_rcnn_loss(y_true, y_pred):\n",
    "    # Compute the binary cross-entropy loss for object detection\n",
    "    rpn_class_loss = tf.keras.losses.binary_crossentropy(y_true['rpn_class'], y_pred['rpn_class'])\n",
    "    \n",
    "    # Compute the smooth L1 loss for bounding box regression\n",
    "    rpn_bbox_loss = tf.keras.losses.smooth_l1(y_true['rpn_bbox'], y_pred['rpn_bbox'])\n",
    "    \n",
    "    # Compute the binary cross-entropy loss for mask prediction\n",
    "    mask_loss = tf.keras.losses.binary_crossentropy(y_true['masks'], y_pred['masks'])\n",
    "    \n",
    "    # Combine the losses and return the total loss\n",
    "    total_loss = rpn_class_loss + rpn_bbox_loss + mask_loss\n",
    "    return total_loss\n",
    "\n",
    "# Define the number of classes (including the background class)\n",
    "num_classes = 2\n",
    "\n",
    "# Define the input image size (assuming square images)\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "# Define the model\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "print(inputs.shape)\n",
    "proposals, rpn_class_logits, rpn_bbox = rpn(inputs)\n",
    "classes, boxes = backbone(inputs)\n",
    "mask_logits = mask_net(inputs, proposals)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=[rpn_class_logits, rpn_bbox, classes, boxes, mask_logits])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=mask_rcnn_loss)\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset, epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55602505796b395471532f5081df3c4fcef54cf33e498b3a10cf72378715fc4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
